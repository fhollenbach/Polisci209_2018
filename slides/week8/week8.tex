% Created 2018-10-22 Mon 08:57
% Intended LaTeX compiler: pdflatex
\documentclass[presentation]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{natbib}
\usepackage[linktocpage,pdfstartview=FitH,colorlinks,
linkcolor=blue,anchorcolor=blue,
citecolor=blue,filecolor=blue,menucolor=blue,urlcolor=blue]{hyperref}
\setbeamertemplate{frame footer}{\insertshortauthor}
\setbeamerfont{page number in head/foot}{size=\tiny}
\setbeamercolor{footline}{fg=gray}
\usepackage{amsmath}
\author{Florian Hollenbach}
\usepackage[english]{isodate}
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usetheme{metropolis}
\usecolortheme{}
\usefonttheme{}
\useinnertheme{}
\useoutertheme{}
\author{Florian Hollenbach}
\date{\today}
\title{Political Science 209 - Fall 2018}
\subtitle{Linear Regression}

\hypersetup{
 pdfauthor={Florian Hollenbach},
 pdftitle={Political Science 209 - Fall 2018},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.3.1 (Org mode 9.1.14)}, 
 pdflang={English}}
\begin{document}

\maketitle



\begin{frame}[label={sec:org616081a}]{In-class Exercise Linear Regression}
Please dowload \emph{intrade08.csv} \& \emph{pres08.csv} from class website

\begin{itemize}
\item Read both data sets into \emph{R}
\item Create data summary for each data sets
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orgcb2fc10}]{Variables in the intrade data}
\begin{itemize}
\item \emph{day}: Date of the session
\item \emph{statename}: Full name of each state (including District of Columbia in 2008)
\item \emph{state}: Abbreviation of each state (including District of Columbia in 2008)
\item \emph{PriceD}: Closing price (predicted vote share) of Democratic Nominee's market
\item \emph{PriceR}: Closing price (predicted vote share) of Republican Nominee's market
\item \emph{VolumeD}: Total session trades of Democratic Party Nominee's market
\item \emph{VolumeR}: Total session trades of Republican Party Nominee's market
\end{itemize}
\end{frame}


\begin{frame}[label={sec:org7e150a9}]{Variables in the pres08 data}
\begin{itemize}
\item \emph{state.name}: Full name of state (only in pres2008)
\item \emph{state}: Two letter state abbreviation
\item \emph{Obama}: Vote percentage for Obama
\item \emph{McCain}: Vote percentage for McCain
\item \emph{EV}: Number of electoral college votes for this state
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org66effeb}]{Combining data sets}
\begin{itemize}
\item First we have to combine the different data sets
\item To do so, we need an identifier that tells \emph{R} which observations to match to each other
\item What could we use?
\end{itemize}

\pause

\alert{state variable}
\end{frame}

\begin{frame}[fragile,label={sec:org23cbed4}]{Combining data sets}
 \begin{itemize}
\item Use \emph{merge()} function
\end{itemize}

\begin{verbatim}
merge(x,y, by =)
intresults08 <- merge(intrade08, pres08, by = "state")
head(intresults08)
\end{verbatim}
\end{frame}


\begin{frame}[label={sec:org5916179}]{Question 1}
Create a \emph{DaysToElection} variable by subtracting the day of the election from each day in the dataset. Now create a \emph{state margin of victory} variable to predict, and a \emph{betting market margin} to predict it with.

\alert{election day in 2008: Nov, 4th}
\end{frame}

\begin{frame}[fragile,shrink=20,label={sec:org9d022db}]{Solution 1}
 \begin{verbatim}
intresults08$DaysToElection
 <- as.Date("2008-11-04") - as.Date(intresults08$day)

intresults08$obama.intmarg <- intresults08$PriceD - intresults08$PriceR
intresults08$obama.actmarg <- intresults08$Obama - intresults08$McCain
\end{verbatim}
\end{frame}


\begin{frame}[label={sec:org2af84e8}]{Question 2}
Considering only the trading \alert{one day from the election}, predict the actual electoral margins from the trading margins using a linear model. Does it predict well? How would you visualize the predictions and the outcomes together? Hint: because we only have one predictor you can use \emph{abline}.
\end{frame}

\begin{frame}[fragile,shrink=20,label={sec:orgd2a885a}]{Solution 2}
 \begin{verbatim}
latest08 <- intresults08[intresults08$DaysToElection == 1,]
int.fit08 <- lm(obama.actmarg ~ obama.intmarg, data = latest08)
coef(int.fit08)
summary(int.fit08)$r.squared
plot(latest08$obama.intmarg, latest08$obama.actmarg,
    xlab="Market's margin for Obama", ylab="Obama margin")
abline(int.fit08)
\end{verbatim}
\end{frame}



\begin{frame}[label={sec:orgcbe01db}]{Question 3}
What would be the prediction for the margin of victory if the InTrade margin was 25? Mark this point on the previous plot.
\end{frame}


\begin{frame}[fragile,shrink=20,label={sec:org89ef846}]{Solution 3}
 \begin{verbatim}
coef(int.fit08)[1] + coef(int.fit08)[2]*25

plot(latest08$obama.intmarg, latest08$obama.actmarg,
    xlab="Market's margin for Obama", ylab="Obama margin")
abline(int.fit08)
points(25,(coef(int.fit08)[1] + coef(int.fit08)[2]*25), col = "red")
\end{verbatim}
\end{frame}




\begin{frame}[shrink=20,label={sec:org869e010}]{Question 4}
Even efficient markets aren’t omniscient. Information comes in about the election every day and the market prices should reflect any change in information that seem to matter to the outcome.

We can examine how and about what the markets change their minds by looking at which states they are confident about, and which they update their ‘opinions’ (i.e. their prices) about. Over the period before the election, let’s see how prices for each state are evolving. We can get a compact summary of price movement by fitting a linear model to Obama’s margin for each state over the 20 days before the election.

We will summarise price movement by the direction (up or down) and rate of change (large or small) of price over time. This is basically also what people in finance do, but they get paid more…

\alert{Start by plotting Obama’s margin in West Virginia against the number of days until the election and modeling the relationship with a linear model. Use the last 20 days. Show the model's predictions on each day and the data. What does this model's slope coefficient tells us about which direction the margin is changing and also how fast it is changing?}
\end{frame}


\begin{frame}[fragile,shrink=20,label={sec:orgd39f9b5}]{Solution 4}
 \begin{verbatim}
stnames <- unique(intresults08$state.name)
recent <- subset(intresults08, subset=(DaysToElection <= 20)
 & (state.name==stnames[1]))

recent.mod <- lm(obama.intmarg ~ DaysToElection, data=recent)
plot(recent$DaysToElection, recent$obama.intmarg,
xlab="Days to election", ylab="Market's Obama margin")
abline(recent.mod)
\end{verbatim}
\end{frame}

\begin{frame}[label={sec:org9530f6d}]{Question 5}
Let's do the same thing for all states and collect the slope coefficients (\(\beta\)'s). How can we modify the code from the answer to the previous question? Then plot the distribution of changes for all states.
\end{frame}

\begin{frame}[fragile,shrink=25,label={sec:orga4c514b}]{Solution 5}
 \begin{verbatim}
stnames <- unique(intresults08$state.name)
change <- rep(NA, length(unique(intresults08$state.name)))
names(change) <- unique(intresults08$state.name)

for(i in 1: length(unique(intresults08$state.name))){
recent <- subset(intresults08, subset=(DaysToElection <= 20)
& (state.name==stnames[i]))

recent.mod <- lm(obama.intmarg ~ DaysToElection, data=recent)
change[i] <- coef(recent.mod)[2]
}
hist(change)
\end{verbatim}
\end{frame}


\begin{frame}[label={sec:orga45e50f}]{Questin 5}
Estimate a linear model using the intrade margin in the average intrade margin in the week before the election to predict vote margin in 2008. How well does the model predict?
\end{frame}

\begin{frame}[fragile,shrink=25,label={sec:orgc581ed2}]{Solution 5}
 \begin{verbatim}
latest08 <- intresults08[intresults08$DaysToElection <8,]
average.Intrade <- tapply(latest08$obama.intmarg, latest08$state, mean)
true.margin <- tapply(latest08$obama.actmarg, latest08$state, mean)

int.fit08 <- lm(true.margin ~ average.Intrade)
coef(int.fit08)
summary(int.fit08)$r.squared

\end{verbatim}
\end{frame}

\begin{frame}[label={sec:org82f7dc2}]{Question 6}
Next, we read in the same data for the 2012 election. Use the linear model created above to create predictions for the margin in 2012. Calculate and plot the prediction error.
\end{frame}

\begin{frame}[fragile,shrink=25,label={sec:org18f3046}]{Solution 6}
 \begin{verbatim}
data2012 <- read.csv("intresults12.csv")
data2012$DaysToElection <- as.Date("2008-11-06") - as.Date(data2012$day)
data2012$obama.intmarg <- data2012$PriceD - data2012$PriceR
data2012$obama.actmarg <- data2012$Obama - data2012$Romney
\end{verbatim}
\end{frame}


\begin{frame}[fragile,shrink=25,label={sec:org6df5d16}]{Solution 6}
 \begin{verbatim}
latest12
<- data2012[data2012$DaysToElection <8,]

average.Intrade12
<- tapply(latest12$obama.intmarg, latest12$state, mean, na.rm = T)

true.margin12
<- tapply(latest12$obama.actmarg, latest12$state, mean, na.rm = T)

prediction
<- coef(int.fit08)[1] + coef(int.fit08)[2]*average.Intrade12

error <- true.margin12 - prediction
hist(error)
\end{verbatim}
\end{frame}


\begin{frame}[label={sec:org6cab27f}]{Linear Regression and RCTs}
Can we estimate regression models on data from experiments?

\pause

\alert{Yes, treatment status as the independent variable (0 or 1)}
\end{frame}



\begin{frame}[label={sec:orgea6265f}]{Linear Regression and RCTs}
\begin{itemize}
\item y = \(\alpha\) + \(\beta\) * treatment + \(\epsilon\)
\end{itemize}


\begin{itemize}
\item What is the interpretation of \(\alpha\) here?
\end{itemize}

\pause

\begin{itemize}
\item What is the interpretation of \(\beta\)?
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org57df42e}]{Linear Regression and RCTs}
\begin{itemize}
\item y = \(\alpha\) + \(\beta\) * treatment + \(\epsilon\)

\item \(\beta\) = average treatment effect

\item The two predicted values are the average outcome under each condition
\end{itemize}

\pause

\begin{itemize}
\item \(\beta\): Predicted change in Y \emph{caused} by increase of \emph{T} by 1
\end{itemize}

\pause

\alert{Remember, generally regression coefficents are not to be interpreted as causal effects!}
\end{frame}

\begin{frame}[fragile,shrink=20,label={sec:org3aea0f2}]{Race and Job Applications}
 \begin{verbatim}
resume <- read.csv("resume.csv")
head(resume)
\end{verbatim}

\begin{verbatim}

  firstname    sex  race call
1   Allison female white    0
2   Kristen female white    0
3   Lakisha female black    0
4   Latonya female black    0
5    Carrie female white    0
6       Jay   male white    0
\end{verbatim}

\begin{itemize}
\item Randomized ``race'' in job applications
\item What is the effect of race on likelyhood of callback?
\end{itemize}

\alert{Marianne Bertrand and Sendhil Mullainathan (American Economic Review 2004)}
\end{frame}

\begin{frame}[fragile,shrink=30,label={sec:org99c9f48}]{Race and Job Applications}
 \begin{verbatim}
mean(resume$call[resume$race == "black"])
mean(resume$call[resume$race == "white"])
mean(resume$call[resume$race == "black"]) - mean(resume$call[resume$race == "white"])
\end{verbatim}

\begin{verbatim}
[1] 0.06447639

[1] 0.09650924

[1] -0.03203285
\end{verbatim}
\end{frame}


\begin{frame}[fragile,shrink=20,label={sec:orgc8f2bb7}]{Race and Job Applications}
 \begin{verbatim}
linear <- lm(call ~ race, data = resume)
coef(linear)
\end{verbatim}

\begin{verbatim}

(Intercept)   racewhite 
 0.06447639  0.03203285
\end{verbatim}

R automatically turns the factor into a dummy (binary) variable


\pause

\begin{itemize}
\item \(\alpha\) is the intercept, when X = 0 (i.e. race is ``black'')
\item \(\beta\) is change in \hat{Y} when X is set to 1 (i.e. race is ``white'')
\end{itemize}
\end{frame}


\begin{frame}[label={sec:orga503693}]{Linear Regression with multiple independent variables}
\begin{eqnarray*}
      Y & = & \alpha + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \epsilon
\end{eqnarray*}

\begin{itemize}
\item principle of regression model stays the same

\item we attempt to draw the best fitting line through a cloud of points (now in multiple dimensions)
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org38b4bb1}]{Linear Regression with multiple independent variables}
We still minimize the sum of the squared residuals:

\begin{eqnarray*}
      \textsf{SSR} & = & \sum_{i=1}^n \hat\epsilon_i^2
    \end{eqnarray*}
\end{frame}


\begin{frame}[label={sec:org3dfc89d}]{Linear Regression with multiple independent variables}
We still minimize the sum of the squared residuals:

\begin{eqnarray*}
      \textsf{SSR} & = & \sum_{i=1}^n \hat\epsilon_i^2 \ = \ \sum_{i=1}^n
                         (Y_i - \hat{Y})^2
    \end{eqnarray*}
\end{frame}


\begin{frame}[label={sec:org5249cdd}]{Linear Regression with multiple independent variables}
We still minimize the sum of the squared residuals:


\begin{eqnarray*}
      \textsf{SSR} & = & \sum_{i=1}^n \hat\epsilon_i^2 \ = \ \sum_{i=1}^n
                         (Y_i - \hat{Y})^2
    \end{eqnarray*}

And thus:

\begin{eqnarray*}

 \textsf{SSR} &=& \sum_{i=1}^n (Y_i - (\hat\alpha + \hat\beta_1 X_{i1} +
                         \hat\beta_2 X_{i2} + \cdots + \hat\beta_p X_{ip}))^2
    \end{eqnarray*}
\end{frame}



\begin{frame}[label={sec:org3f61f3b}]{Linear Regression with multiple independent variables}
Interpretation:

\begin{itemize}
\item \(\alpha\): Intercept or \(\hat{y}\) when all \(X_{p} = 0\)
\end{itemize}

\pause

\begin{itemize}
\item \(\beta_{p}\): Slope of predictor \(X_{p}\)
\end{itemize}

\pause

\begin{itemize}
\item \(\beta_{p}\): Predicted change in \(\hat{Y}\) when \(X_{p}\) increases by \(1\) \alert{and} all other predictors \alert{are held constant}!
\end{itemize}
\end{frame}



\begin{frame}[label={sec:orge68d71f}]{Linear Regression with multiple independent variables}
\begin{itemize}
\item \(\beta_{p}\): Predicted change in \(\hat{Y}\) when \(X_{p}\) increases by \(1\) \alert{and} all other predictors \alert{are held constant}!

\item we can use the multiple regression to \alert{control for confounders}
\end{itemize}

\pause

\begin{itemize}
\item impact of each individual predictor when the other predictors do not change

\item Example: Association between income and child mortality when regime type is not changing
\end{itemize}
\end{frame}

\begin{frame}[fragile,label={sec:orgec9b1d7}]{Linear Regression with multiple independent variables in \emph{R}}
 \begin{verbatim}
result <- lm(y ~ x1 + x2 + x3 + x4, data = data)
coef(result)
\end{verbatim}
\end{frame}



\begin{frame}[fragile,label={sec:org8bde946}]{Linear Regression with multiple independent variables in \emph{R}}
 \begin{verbatim}
data <- read.csv("bivariate_data.csv")
data2010 <- subset(data, Year == 2010)
bivar <- lm(Child.Mortality ~ log(GDP), data = data)
coef(bivar)
summary(bivar)$r.squared
\end{verbatim}
\end{frame}

\begin{frame}[fragile,label={sec:orgc642ab1}]{Linear Regression with multiple independent variables in \emph{R}}
 \begin{verbatim}
data <- read.csv("bivariate_data.csv")
data2010 <- subset(data, Year == 2010)
bivar <- lm(Child.Mortality ~ log(GDP), data = data2010)
coef(bivar)
summary(bivar)$r.squared
\end{verbatim}

\begin{verbatim}

(Intercept)    log(GDP) 
  276.58162   -26.12717

[1] 0.586953
\end{verbatim}
\end{frame}




\begin{frame}[fragile,shrink=20,label={sec:orgf416f8d}]{Linear Regression with multiple independent variables in \emph{R}}
 \begin{verbatim}
data <- read.csv("bivariate_data.csv")
data2010 <- subset(data, Year == 2010)
multiple <- lm(Child.Mortality ~ log(GDP) + PolityIV, data = data2010)
coef(multiple)
summary(multiple)$r.squared
\end{verbatim}
\end{frame}



\begin{frame}[fragile,shrink=20,label={sec:org0c23239}]{Linear Regression with multiple independent variables in \emph{R}}
 \begin{verbatim}
data <- read.csv("bivariate_data.csv")
data2010 <- subset(data, Year == 2010)
multiple <- lm(Child.Mortality ~ log(GDP) + PolityIV, data = data2010)
coef(multiple)
summary(multiple)$r.squared
\end{verbatim}

\begin{verbatim}

(Intercept)    log(GDP)    PolityIV 
 277.845620  -25.641789   -1.029062

[1] 0.6113747
\end{verbatim}
\end{frame}



\begin{frame}[fragile,shrink=20,label={sec:org5237c62}]{Linear Regression with multiple independent variables in \emph{R}}
 \begin{verbatim}
data <- read.csv("bivariate_data.csv")
data2010 <- subset(data, Year == 2010)
multiple <- lm(Child.Mortality ~ log(GDP) + PolityIV, data = data2010)
coef(multiple)
coef(bivar)
\end{verbatim}

\begin{verbatim}

(Intercept)    log(GDP)    PolityIV 
 277.845620  -25.641789   -1.029062

(Intercept)    log(GDP) 
  276.58162   -26.12717
\end{verbatim}
\end{frame}


\begin{frame}[label={sec:org0986a5f}]{Linear Regression with multiple independent variables in \emph{R}}
\begin{itemize}
\item In multiple regression models we want to adjust the goodness of fit statistic by the number of variables included
\item This is done via the degrees of freedom (DF) adjustment:
\end{itemize}

\begin{eqnarray*}
      \textsf{adjusted} R^{2} & = & 1 - \frac{SSR /(n-p-1)}{TSS/(n-1)}
    \end{eqnarray*}
\end{frame}

\begin{frame}[fragile,shrink=20,label={sec:org2938e7e}]{Linear Regression with multiple independent variables in \emph{R}}
 \begin{verbatim}
data <- read.csv("bivariate_data.csv")
data2010 <- subset(data, Year == 2010)
multiple <- lm(Child.Mortality ~ log(GDP) + PolityIV, data = data2010)
coef(multiple)
summary(multiple)$r.squared
summary(multiple)$adj.r.squared
\end{verbatim}
\end{frame}

\begin{frame}[fragile,shrink=20,label={sec:orgcec6cbc}]{Linear Regression with multiple independent variables in \emph{R}}
 \begin{verbatim}
data <- read.csv("bivariate_data.csv")
data2010 <- subset(data, Year == 2010)
multiple <- lm(Child.Mortality ~ log(GDP) + PolityIV, data = data2010)
coef(multiple)
summary(multiple)$r.squared
summary(multiple)$adj.r.squared
\end{verbatim}

\begin{verbatim}

(Intercept)    log(GDP)    PolityIV 
 277.845620  -25.641789   -1.029062

[1] 0.6113747

[1] 0.6061582
\end{verbatim}
\end{frame}
\end{document}